{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1-izC4ZKHJ9"
   },
   "source": [
    "<h1 style=\"text-align: center\">\n",
    "Machine Learning </br>\n",
    "MLE & MAP in Python\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font>\n",
    "    <h1 style=\"text-align: center\">\n",
    "        AmirReza Azari <br>\n",
    "        99101087\n",
    "    </h1>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhTuYwHYbE_1"
   },
   "source": [
    "## Objective\n",
    "This exercise will help you gain a deeper understanding of, and insights into, Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP) estimation$\\textit{Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP) }$ :) \\\\\n",
    "Let’s say you have a barrel of apples that are all different sizes. You pick an apple at random, and you want to know its weight. Unfortunately, all you have is a broken scale. answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSaLb6AYa9DJ"
   },
   "source": [
    "1) For the sake of this section, lets imagine a farmer tells you that the scale returns the weight of the object with an error of +/- a standard deviation of 5g. We can describe this mathematically as:\n",
    "$$\n",
    "measurement = weight + \\mathcal{N}(0, 5g)\n",
    "$$\n",
    "You can weigh the apple as many times as you want, so weigh it 100 times.\n",
    "plot its histogram of your 100 measurements. (y axis is the counts and x-axis is the measured weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hPMnHTcia07a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62.23505092 65.09601766 66.36858699 66.46864905 66.84900827 66.93051076\n",
      " 67.31878157 67.72817163 67.89991031 68.7360232  68.8258709  69.09683908\n",
      " 69.1742508  69.64623689 69.75723517 70.1136106  70.46350818 70.52266719\n",
      " 70.56107126 70.64601425 70.7295213  70.93426859 71.2891749  71.57594955\n",
      " 71.63769776 71.82838953 72.10575168 72.44597431 72.45173909 72.80962849\n",
      " 72.98411527 72.99109532 73.06336591 73.18629417 73.20223419 73.26043925\n",
      " 73.44223734 73.48848625 73.9362986  73.97420868 74.06408075 74.10037582\n",
      " 74.24321396 74.48390574 74.85908886 75.0525001  75.22879259 75.25972698\n",
      " 75.28082671 75.33258611 75.60837508 75.63456046 75.64491455 75.72021786\n",
      " 75.77473713 75.78174485 75.88713071 76.04137489 76.51235949 76.56533851\n",
      " 76.66837164 76.78183199 76.8908126  76.93451249 77.00078604 77.00994682\n",
      " 77.01170821 77.05299251 77.14165935 77.21931616 77.31391128 77.3283122\n",
      " 78.26809298 78.53286584 78.64545281 78.80518863 78.88745178 79.32218099\n",
      " 79.50413243 79.75044209 79.88319518 79.89368992 80.27225863 80.69700342\n",
      " 80.89389786 81.01189924 81.11222535 81.1514534  82.27136753 82.34679385\n",
      " 82.44126097 82.47039537 82.66389607 83.82026173 83.92935247 84.33778995\n",
      " 84.47944588 84.75387698 86.204466   86.34877312]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "# Based on what you said in last cell: \"The average apples is between 70-100 g\", I considered 75g for avg and add 75 to the numbers of the \"data\" which has normal numbers.\n",
    "weight = 75\n",
    "data = np.random.normal(0, 5, 100) + 75\n",
    "data.sort()\n",
    "print(data)\n",
    "\n",
    "plt.hist(data, bins=20, color='SeaGreen')\n",
    "plt.xlabel(\"measured weight\", color='GreenYellow') \n",
    "plt.ylabel(\"counts\", color='GreenYellow') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HD9Mqy-bcPi5"
   },
   "source": [
    "2) Find the average weight of the apple.\n",
    "Is it a good guess? state your reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xlCBTC0lcPKa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.29904007767243"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"Salmon\">\n",
    "    Average is 75.299 ~ 75.3 by 100 times weighting.<br>\n",
    "    Actual weight is 75.<br>\n",
    "    So it's a good guess.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-P9PJuKcrbq"
   },
   "source": [
    "3) we are going to use grid approximation for calculating the MLE. here is the link if you wnat to get more fimilar with this technique:\n",
    "https://www.bayesrulesbook.com/chapter-6\n",
    "\n",
    "Our end goal is to find the weight of the apple, given the data we have. To formulate it in a Bayesian way: We’ll ask what is the probability of the apple having weight, $w$, given the measurements we took, $X$. And, because we're formulating this in a Bayesian way, we use Bayes’ Law to find the answer:\n",
    "\n",
    "$$\n",
    "P(w|X) = \\frac{P(X|w)P(w)}{P(X)}\n",
    "$$\n",
    "\n",
    "If we make no assumptions about the initial weight of our apple, then we can drop $P(w)$. We’ll say all sizes of apples are equally likely (we’ll revisit this assumption in the MAP approximation).\n",
    "\n",
    "Furthermore, we’ll drop $P(X)$ - the probability of seeing our data. This is a normalization constant and will be important if we do want to know the probabilities of apple weights. But, for right now, our end goal is to only to find the most probable weight. $P(X)$ is independent of $w$, so we can drop it if we’re doing relative comparisons.\n",
    "\n",
    "This leaves us with $P(X|w)$, our likelihood, as in, what is the likelihood that we would see the data, $X$, given an apple of weight $w$. If we maximize this, we maximize the probability that we will guess the right weight.\n",
    "\n",
    "The grid approximation is probably the simplest way to do this. Basically, we’ll systematically step through different weight guesses, and compare what it would look like if this hypothetical weight were to generate data. We’ll compare this hypothetical data to our real data and pick the one that matches the best.\n",
    "\n",
    "To formulate this mathematically:\n",
    "\n",
    "For each of these guesses, we’re asking \"what is the probability that the data we have, came from the distribution that our weight guess would generate\". Because each measurement is independent from another, we can break the above equation down into finding the probability on a per measurement basis:\n",
    "\n",
    "$$\n",
    "P(X|w) = \\prod_{i}^{N} p(x_i|w)\n",
    "$$\n",
    "\n",
    "So, if we multiply the probability that we would see each individual data point - given our weight guess - then we can find one number comparing our weight guess to all of our data.\n",
    "\n",
    "The peak in the likelihood is the weight of the apple.\n",
    "\n",
    "To make it computationally easier,\n",
    "\n",
    "$$\n",
    "\\log P(X|w) = \\log \\prod_{i}^{N} p(x_i|w) = \\sum_{i}^{N} \\log p(d_i|w)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "a) Why did we use log likelihood? Is it ok to do so?\n",
    "\n",
    "b) do the grid approximation and complete the cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"LightBlue\">\n",
    "      a) Why did we use log likelihood? Is it ok to do so?<br>\n",
    "</font>\n",
    "<font color=\"Salmon\">\n",
    "      Log is an increasing function, so it's ok.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9NnWmxzTiRfr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"data\" dataframe:\n",
      " [62.23505092 65.09601766 66.36858699 66.46864905 66.84900827 66.93051076\n",
      " 67.31878157 67.72817163 67.89991031 68.7360232  68.8258709  69.09683908\n",
      " 69.1742508  69.64623689 69.75723517 70.1136106  70.46350818 70.52266719\n",
      " 70.56107126 70.64601425 70.7295213  70.93426859 71.2891749  71.57594955\n",
      " 71.63769776 71.82838953 72.10575168 72.44597431 72.45173909 72.80962849\n",
      " 72.98411527 72.99109532 73.06336591 73.18629417 73.20223419 73.26043925\n",
      " 73.44223734 73.48848625 73.9362986  73.97420868 74.06408075 74.10037582\n",
      " 74.24321396 74.48390574 74.85908886 75.0525001  75.22879259 75.25972698\n",
      " 75.28082671 75.33258611 75.60837508 75.63456046 75.64491455 75.72021786\n",
      " 75.77473713 75.78174485 75.88713071 76.04137489 76.51235949 76.56533851\n",
      " 76.66837164 76.78183199 76.8908126  76.93451249 77.00078604 77.00994682\n",
      " 77.01170821 77.05299251 77.14165935 77.21931616 77.31391128 77.3283122\n",
      " 78.26809298 78.53286584 78.64545281 78.80518863 78.88745178 79.32218099\n",
      " 79.50413243 79.75044209 79.88319518 79.89368992 80.27225863 80.69700342\n",
      " 80.89389786 81.01189924 81.11222535 81.1514534  82.27136753 82.34679385\n",
      " 82.44126097 82.47039537 82.66389607 83.82026173 83.92935247 84.33778995\n",
      " 84.47944588 84.75387698 86.204466   86.34877312]\n",
      "\n",
      "\"wight_grid\" dataframe:\n",
      " [  0.           2.04081633   4.08163265   6.12244898   8.16326531\n",
      "  10.20408163  12.24489796  14.28571429  16.32653061  18.36734694\n",
      "  20.40816327  22.44897959  24.48979592  26.53061224  28.57142857\n",
      "  30.6122449   32.65306122  34.69387755  36.73469388  38.7755102\n",
      "  40.81632653  42.85714286  44.89795918  46.93877551  48.97959184\n",
      "  51.02040816  53.06122449  55.10204082  57.14285714  59.18367347\n",
      "  61.2244898   63.26530612  65.30612245  67.34693878  69.3877551\n",
      "  71.42857143  73.46938776  75.51020408  77.55102041  79.59183673\n",
      "  81.63265306  83.67346939  85.71428571  87.75510204  89.79591837\n",
      "  91.83673469  93.87755102  95.91836735  97.95918367 100.        ]\n",
      "\n",
      "Maximum Likelihood Estimate (MLE) Weight in \"data\" dataframe: 75.51020408163265\n",
      "\n",
      "Maximum Likelihood Estimate (MLE) Weight in \"weight_grid\" dataframe: 48.9795918367347\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "\"\"\"\n",
    "# Calculate the maximum likelihood estimate of a parameter in a normal distribution.\n",
    "# First calculate the log likelihoods for a range of weight guesses.\n",
    "# For each weight guess, assume that the data comes from a normal distribution with that mean and a standard deviation of 10.\n",
    "# Then calculate the log of the probability density function (pdf) of the data under this assumption.\n",
    "# The sum of these log pdf values is the total log likelihood for that weight guess.\n",
    "# After calculating the log likelihoods for all weight guesses, find the weight guess with the maximum log likelihood.\n",
    "# This is the maximum likelihood estimate of the weight.\n",
    "\"\"\"\n",
    "\n",
    "weight_grid = np.linspace(0, 100)\n",
    "print(\"\\\"data\\\" dataframe:\\n\",data)\n",
    "print(\"\\n\\\"wight_grid\\\" dataframe:\\n\",weight_grid)\n",
    "\n",
    "mle_weight = 0\n",
    "mle_likelihood = float('-inf')\n",
    "for w in weight_grid:\n",
    "    likelihood = np.sum(norm.logpdf(data, w, 10))\n",
    "    if likelihood > mle_likelihood:\n",
    "        mle_likelihood = likelihood\n",
    "        mle_weight = w\n",
    "print(\"\\nMaximum Likelihood Estimate (MLE) Weight in \\\"data\\\" dataframe:\", mle_weight)\n",
    "\n",
    "mle_weight = 0\n",
    "mle_likelihood = float('-inf')\n",
    "for w in weight_grid:\n",
    "    likelihood = np.sum(norm.logpdf(weight_grid, w, 10))\n",
    "    if likelihood > mle_likelihood:\n",
    "        mle_likelihood = likelihood\n",
    "        mle_weight = w\n",
    "print(\"\\nMaximum Likelihood Estimate (MLE) Weight in \\\"weight_grid\\\" dataframe:\", mle_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NN3lt2npcc2S"
   },
   "source": [
    "Play around with the code and try to answer the following questions regarding MLE and MAP. You can draw plots to visualize as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ezcWTpNQamCL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE = 100\n",
      "PRIOR = INVGAMMA\n",
      "SCALE_ERR =  5, N_MEASURMENTS =  10\n",
      "Average measurement: 85.014 g\n",
      "Maximum Likelihood estimate: 85.424 g\n",
      "Maximum A Posterior estimate: 51.253 g\n",
      "The true weight of the apple was: 86.488 g\n",
      "\n",
      "******************************************************\n",
      "\n",
      "SIZE = 50\n",
      "PRIOR = INVGAMMA\n",
      "SCALE_ERR =  5, N_MEASURMENTS =  10\n",
      "Average measurement: 85.014 g\n",
      "Maximum Likelihood estimate: 86.286 g\n",
      "Maximum A Posterior estimate: 49.735 g\n",
      "The true weight of the apple was: 86.488 g\n",
      "\n",
      "******************************************************\n",
      "\n",
      "SIZE = 100\n",
      "PRIOR = INVGAMMA\n",
      "SCALE_ERR =  5, N_MEASURMENTS =  20\n",
      "Average measurement: 85.797 g\n",
      "Maximum Likelihood estimate: 85.424 g\n",
      "Maximum A Posterior estimate: 51.253 g\n",
      "The true weight of the apple was: 86.488 g\n",
      "\n",
      "******************************************************\n",
      "\n",
      "SIZE = 100\n",
      "PRIOR = BETA\n",
      "SCALE_ERR =  5, N_MEASURMENTS =  20\n",
      "Average measurement: 85.797 g\n",
      "Maximum Likelihood estimate: 85.424 g\n",
      "Maximum A Posterior estimate: 85.424 g\n",
      "The true weight of the apple was: 86.488 g\n",
      "\n",
      "******************************************************\n",
      "\n",
      "SIZE = 50\n",
      "PRIOR = BETA\n",
      "SCALE_ERR =  5, N_MEASURMENTS =  20\n",
      "Average measurement: 85.797 g\n",
      "Maximum Likelihood estimate: 86.286 g\n",
      "Maximum A Posterior estimate: 86.286 g\n",
      "The true weight of the apple was: 86.488 g\n",
      "\n",
      "******************************************************\n",
      "\n",
      "SIZE = 50\n",
      "PRIOR = INVGAMMA\n",
      "SCALE_ERR =  5, N_MEASURMENTS =  20\n",
      "Average measurement: 85.797 g\n",
      "Maximum Likelihood estimate: 86.286 g\n",
      "Maximum A Posterior estimate: 49.735 g\n",
      "The true weight of the apple was: 86.488 g\n",
      "\n",
      "******************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, invgamma, beta\n",
    "\n",
    "\n",
    "# The barrel of apples\n",
    "# The average apples is between 70-100 g\n",
    "BARREL = np.random.normal(loc=85, scale=20, size=100)\n",
    "# Grid\n",
    "WEIGHT_GUESSES = np.linspace(1, 200, 100)\n",
    "ERROR_GUESSES = np.linspace(.1, 50, 100)\n",
    "\n",
    "# NOTE: Try changing the scale error\n",
    "# in practice, you would not know this number\n",
    "SCALE_ERR = 5\n",
    "\n",
    "# NOTE: Try changing the number of measurements taken\n",
    "N_MEASURMENTS = 10\n",
    "\n",
    "# NOTE: Try changing the prior values and distributions\n",
    "PRIOR_WEIGHT = norm(50, 1).logpdf(WEIGHT_GUESSES)\n",
    "PRIOR_ERR = invgamma(4).logpdf(ERROR_GUESSES)\n",
    "\n",
    "LOG_PRIOR_GRID = np.add.outer(PRIOR_ERR, PRIOR_WEIGHT)\n",
    "\n",
    "\n",
    "def read_scale(apple):\n",
    "    return apple + np.random.normal(loc=0, scale=SCALE_ERR)\n",
    "\n",
    "\n",
    "def get_log_likelihood_grid(measurments):\n",
    "    log_liklelihood = [\n",
    "        [\n",
    "            norm(weight_guess, error_guess).logpdf(measurments).sum()\n",
    "            for weight_guess in WEIGHT_GUESSES\n",
    "        ]\n",
    "        for error_guess in ERROR_GUESSES\n",
    "    ]\n",
    "    return np.asarray(log_liklelihood)\n",
    "\n",
    "\n",
    "def get_mle(measurments):\n",
    "    \"\"\"\n",
    "    Calculate the log-likelihood for each measurement in the grid.\n",
    "    Find the index of the maximum log-likelihood in the grid.\n",
    "    Return the weight guess corresponding to the maximum log-likelihood.\n",
    "    \"\"\"\n",
    "    grid = get_log_likelihood_grid(measurments)\n",
    "    max_index = np.unravel_index(grid.argmax(), grid.shape)\n",
    "    return WEIGHT_GUESSES[max_index[1]]\n",
    "\n",
    "\n",
    "def get_map(measurements):\n",
    "    \"\"\"\n",
    "    Calculate the log-likelihood for each measurement in the grid.\n",
    "    Add the log prior to the log likelihood to get the log posterior.\n",
    "    Find the index of the maximum log posterior in the grid.\n",
    "    Return the weight guess corresponding to the maximum log posterior.\n",
    "    \"\"\"\n",
    "    grid = get_log_likelihood_grid(measurements)\n",
    "    grid += LOG_PRIOR_GRID\n",
    "    max_index = np.unravel_index(grid.argmax(), grid.shape)\n",
    "    return WEIGHT_GUESSES[max_index[1]]\n",
    "\n",
    "# Pick an apple at random\n",
    "apple = np.random.choice(BARREL)\n",
    "\n",
    "# weight the apple\n",
    "measurments = np.asarray([read_scale(apple) for _ in range(N_MEASURMENTS)])\n",
    "print(\"SIZE = 100\")\n",
    "print(\"PRIOR = INVGAMMA\")\n",
    "print(\"SCALE_ERR = \", SCALE_ERR, end=\", \")\n",
    "print(\"N_MEASURMENTS = \", N_MEASURMENTS)\n",
    "print(f\"Average measurement: {measurments.mean():.3f} g\")\n",
    "print(f\"Maximum Likelihood estimate: {get_mle(measurments):.3f} g\")\n",
    "print(f\"Maximum A Posterior estimate: {get_map(measurments):.3f} g\")\n",
    "print(f\"The true weight of the apple was: {apple:.3f} g\")\n",
    "\n",
    "print(\"\\n******************************************************\\n\")\n",
    "\n",
    "WEIGHT_GUESSES = np.linspace(1, 200, 50)\n",
    "ERROR_GUESSES = np.linspace(.1, 50, 50)\n",
    "PRIOR_WEIGHT = norm(50, 1).logpdf(WEIGHT_GUESSES)\n",
    "PRIOR_ERR = invgamma(4).logpdf(ERROR_GUESSES)\n",
    "LOG_PRIOR_GRID = np.add.outer(PRIOR_ERR, PRIOR_WEIGHT)\n",
    "print(\"SIZE = 50\")\n",
    "print(\"PRIOR = INVGAMMA\")\n",
    "print(\"SCALE_ERR = \", SCALE_ERR, end=\", \")\n",
    "print(\"N_MEASURMENTS = \", N_MEASURMENTS)\n",
    "print(f\"Average measurement: {measurments.mean():.3f} g\")\n",
    "print(f\"Maximum Likelihood estimate: {get_mle(measurments):.3f} g\")\n",
    "print(f\"Maximum A Posterior estimate: {get_map(measurments):.3f} g\")\n",
    "print(f\"The true weight of the apple was: {apple:.3f} g\")\n",
    "\n",
    "print(\"\\n******************************************************\\n\")\n",
    "\n",
    "SCALE_ERR = 5\n",
    "N_MEASURMENTS = 20\n",
    "WEIGHT_GUESSES = np.linspace(1, 200, 100)\n",
    "ERROR_GUESSES = np.linspace(.1, 50, 100)\n",
    "PRIOR_WEIGHT = norm(50, 1).logpdf(WEIGHT_GUESSES)\n",
    "PRIOR_ERR = invgamma(4).logpdf(ERROR_GUESSES)\n",
    "LOG_PRIOR_GRID = np.add.outer(PRIOR_ERR, PRIOR_WEIGHT)\n",
    "measurments = np.asarray([read_scale(apple) for _ in range(N_MEASURMENTS)])\n",
    "print(\"SIZE = 100\")\n",
    "print(\"PRIOR = INVGAMMA\")\n",
    "print(\"SCALE_ERR = \", SCALE_ERR, end=\", \")\n",
    "print(\"N_MEASURMENTS = \", N_MEASURMENTS)\n",
    "print(f\"Average measurement: {measurments.mean():.3f} g\")\n",
    "print(f\"Maximum Likelihood estimate: {get_mle(measurments):.3f} g\")\n",
    "print(f\"Maximum A Posterior estimate: {get_map(measurments):.3f} g\")\n",
    "print(f\"The true weight of the apple was: {apple:.3f} g\")\n",
    "\n",
    "print(\"\\n******************************************************\\n\")\n",
    "\n",
    "PRIOR_ERR = beta(4, 4).logpdf(ERROR_GUESSES)\n",
    "LOG_PRIOR_GRID = np.add.outer(PRIOR_ERR, PRIOR_WEIGHT)\n",
    "SCALE_ERR = 5\n",
    "N_MEASURMENTS = 20\n",
    "print(\"SIZE = 100\")\n",
    "print(\"PRIOR = BETA\")\n",
    "print(\"SCALE_ERR = \", SCALE_ERR, end=\", \")\n",
    "print(\"N_MEASURMENTS = \", N_MEASURMENTS)\n",
    "print(f\"Average measurement: {measurments.mean():.3f} g\")\n",
    "print(f\"Maximum Likelihood estimate: {get_mle(measurments):.3f} g\")\n",
    "print(f\"Maximum A Posterior estimate: {get_map(measurments):.3f} g\")\n",
    "print(f\"The true weight of the apple was: {apple:.3f} g\")\n",
    "\n",
    "print(\"\\n******************************************************\\n\")\n",
    "\n",
    "WEIGHT_GUESSES = np.linspace(1, 200, 50)\n",
    "ERROR_GUESSES = np.linspace(.1, 50, 50)\n",
    "PRIOR_WEIGHT = norm(50, 1).logpdf(WEIGHT_GUESSES)\n",
    "PRIOR_ERR = beta(4, 4).logpdf(ERROR_GUESSES)\n",
    "LOG_PRIOR_GRID = np.add.outer(PRIOR_ERR, PRIOR_WEIGHT)\n",
    "SCALE_ERR = 5\n",
    "N_MEASURMENTS = 20\n",
    "print(\"SIZE = 50\")\n",
    "print(\"PRIOR = BETA\")\n",
    "print(\"SCALE_ERR = \", SCALE_ERR, end=\", \")\n",
    "print(\"N_MEASURMENTS = \", N_MEASURMENTS)\n",
    "print(f\"Average measurement: {measurments.mean():.3f} g\")\n",
    "print(f\"Maximum Likelihood estimate: {get_mle(measurments):.3f} g\")\n",
    "print(f\"Maximum A Posterior estimate: {get_map(measurments):.3f} g\")\n",
    "print(f\"The true weight of the apple was: {apple:.3f} g\")\n",
    "\n",
    "print(\"\\n******************************************************\\n\")\n",
    "\n",
    "WEIGHT_GUESSES = np.linspace(1, 200, 50)\n",
    "ERROR_GUESSES = np.linspace(.1, 50, 50)\n",
    "PRIOR_WEIGHT = norm(50, 1).logpdf(WEIGHT_GUESSES)\n",
    "PRIOR_ERR = invgamma(4).logpdf(ERROR_GUESSES)\n",
    "LOG_PRIOR_GRID = np.add.outer(PRIOR_ERR, PRIOR_WEIGHT)\n",
    "SCALE_ERR = 5\n",
    "N_MEASURMENTS = 20\n",
    "print(\"SIZE = 50\")\n",
    "print(\"PRIOR = INVGAMMA\")\n",
    "print(\"SCALE_ERR = \", SCALE_ERR, end=\", \")\n",
    "print(\"N_MEASURMENTS = \", N_MEASURMENTS)\n",
    "print(f\"Average measurement: {measurments.mean():.3f} g\")\n",
    "print(f\"Maximum Likelihood estimate: {get_mle(measurments):.3f} g\")\n",
    "print(f\"Maximum A Posterior estimate: {get_map(measurments):.3f} g\")\n",
    "print(f\"The true weight of the apple was: {apple:.3f} g\")\n",
    "\n",
    "print(\"\\n******************************************************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LI_541TpetKk"
   },
   "source": [
    "<h3><i><i> Questions</h3>\n",
    "1.\n",
    "How sensitive is the MAP measurement to the choice of prior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"Salmon\">\n",
    "      It's very sensetive based on last part.<br>\n",
    "       For example Beta improve the guess and it's really better than invgamma.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMV-wgYXes_O"
   },
   "source": [
    "<h3><i><i></h3>\n",
    "2. How sensitive is the MLE and MAP answer to the grid size?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"Salmon\">\n",
    "      MLE is more sensetive to grid size than MAP.<br>\n",
    "      Generally if we increase the size of the grid, the estimation with be much closer to the real weight of the apple.\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
